<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<style>
body {
  margin: 0;
  padding: 0;
  background-color: #f6f8fa;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
}

.wrapper {
  max-width: 760px;
  margin: 40px auto;
  background: #ffffff;
  padding: 48px;
  border-radius: 12px;
  box-shadow: 0 8px 24px rgba(0,0,0,0.06);
  line-height: 1.7;
  color: #111827;
}

h1 {
  font-size: 34px;
  margin-bottom: 12px;
  line-height: 1.3;
}

.meta {
  font-size: 14px;
  color: #6b7280;
  margin-bottom: 20px;
}

.description {
  font-size: 18px;
  color: #374151;
  margin-bottom: 28px;
}

.tags {
  margin-bottom: 32px;
}

.tag {
  display: inline-block;
  background: #eef2ff;
  color: #3730a3;
  padding: 6px 12px;
  border-radius: 999px;
  font-size: 13px;
  margin-right: 6px;
  margin-bottom: 6px;
}

h2 {
  margin-top: 36px;
  font-size: 24px;
  border-bottom: 1px solid #e5e7eb;
  padding-bottom: 6px;
}

h3 {
  margin-top: 28px;
  font-size: 18px;
}

p {
  margin: 16px 0;
}

code {
  background: #f3f4f6;
  padding: 4px 8px;
  border-radius: 6px;
  font-size: 14px;
  font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
}

pre {
  background: #0f172a;
  color: #f8fafc;
  padding: 18px;
  border-radius: 10px;
  overflow-x: auto;
  font-size: 14px;
  line-height: 1.6;
}

pre code {
  background: none;
  padding: 0;
  color: inherit;
}

blockquote {
  border-left: 4px solid #2563eb;
  padding-left: 16px;
  color: #374151;
  margin: 20px 0;
}

table {
  border-collapse: collapse;
  width: 100%;
  margin: 24px 0;
}

th, td {
  border: 1px solid #e5e7eb;
  padding: 10px;
  text-align: left;
  font-size: 14px;
}

th {
  background: #f9fafb;
}

hr {
  margin: 48px 0;
  border: none;
  border-top: 1px solid #e5e7eb;
}

.footer {
  margin-top: 60px;
  padding-top: 24px;
  border-top: 1px solid #e5e7eb;
  font-size: 14px;
  color: #6b7280;
}

.footer strong {
  color: #111827;
}

.footer a {
  color: #2563eb;
  text-decoration: none;
}

.footer a:hover {
  text-decoration: underline;
}

.signature {
  margin-top: 8px;
  line-height: 1.6;
}
</style>
</head>

<body>

<div class="wrapper">

  <!-- Title -->
  <h1>Designing a Scalable Eventâ€‘Driven Order Processing Pipeline with NestJS, PostgreSQL Logical Replication, and LLMâ€‘Powered Fraud Detection</h1>

  <!-- Meta -->
  <div class="meta">
    ðŸ“… 2026-02-22
  </div>

  <!-- Description -->
  <div class="description">
    An inâ€‘depth exploration of building a highâ€‘throughput, faultâ€‘tolerant order pipeline that combines NestJS microservices, PostgreSQL logical replication for strong consistency, and large language model inference for realâ€‘time fraud scoring.
  </div>

  <!-- Tags -->
  <div class="tags">
    <span class="tag">backend-architecture</span>
    <span class="tag">postgresql</span>
    <span class="tag">nestjs</span>
    <span class="tag">npm</span>
    <span class="tag">ai-ml</span>
    <span class="tag">event-driven</span>
    <span class="tag">distributed-systems</span>
  </div>

  <!-- Blog Content (HTML) -->
  <p>In many eâ€‘commerce platforms the order lifecycle must survive spikes, network partitions, and malicious activity while preserving a single source of truth. A typical monolith quickly becomes a bottleneck when order volume grows beyond a few hundred requests per second, and adding fraud checks often introduces latency that harms conversion.</p>
<p>By decomposing the domain into eventâ€‘driven microservices, we can isolate concerns, scale each component independently, and keep the system resilient to failures. This article walks through a concrete implementation that combines NestJS for the service layer, Kafka as the event backbone, PostgreSQL logical replication for strong consistency, and a dedicated LLM inference service that scores transactions in real time.</p>
<p>The design follows a hybrid CQRS pattern: writeâ€‘side services emit domain events, while readâ€‘side projections are materialized from the replicated PostgreSQL stream. Fraud detection runs as a sidecar that consumes the same event stream, enriches the payload with a risk score, and publishes a verdict event that downstream services act upon.</p>
<h2 id="hybridcqrsarchitecturewithnestjsandkafka">Hybrid CQRS Architecture with NestJS and Kafka</h2>
<p>NestJS provides a modular framework that aligns naturally with microservice boundaries. Each bounded contextâ€”Order, Payment, Fraudâ€”exposes a thin HTTP gateway and a Kafka client that publishes events to a shared topic namespace.</p>
<pre><code>// order.service.ts (NestJS)
import { Injectable } from '@nestjs/common';
import { KafkaProducer } from '@nestjs/microservices';

@Injectable()
export class OrderService {
  constructor(private readonly producer: KafkaProducer) {}

  async placeOrder(dto: CreateOrderDto) {
    const order = await this.saveToPostgres(dto);
    await this.producer.send({
      topic: 'orders.created',
      messages: [{ key: String(order.id), value: JSON.stringify(order) }],
    });
    return order;
  }
}
</code></pre>
<p>The command side writes to the primary PostgreSQL instance. The event payload mirrors the persisted row, ensuring that downstream consumers can reconstruct state without additional lookups. Kafka guarantees ordered delivery per partition, which maps to a sharded key space (e.g., customer ID).</p>
<h2 id="postgresqllogicalreplicationforstrongconsistency">PostgreSQL Logical Replication for Strong Consistency</h2>
<p>Logical replication streams rowâ€‘level changes as WAL messages that can be consumed by any PostgreSQL subscriber. In this architecture the readâ€‘side services subscribe to the same publication, materializing view tables that power API queries.</p>
<pre><code>-- publisher (primary)
CREATE PUBLICATION order_pub FOR TABLE orders, payments;

-- subscriber (readâ€‘side replica)
CREATE SUBSCRIPTION order_sub
  CONNECTION 'host=replica port=5432 dbname=shop user=replicator password=******'
  PUBLICATION order_pub;
</code></pre>
<p>Because replication is logical, the subscriber can apply transformations (e.g., column renames) without affecting the primary. The lag is measured in milliseconds under typical load, which satisfies the latency budget for fraud scoring.</p>
<h2 id="llmpoweredfrauddetectionservice">LLMâ€‘Powered Fraud Detection Service</h2>
<p>A separate container runs an inference server (e.g., TensorRTâ€‘accelerated LLM) exposing a gRPC endpoint. The Fraud microservice subscribes to <code>orders.created</code>, enriches the event with contextual data, and forwards it to the inference service.</p>
<pre><code>// fraud.consumer.ts
import { Injectable } from '@nestjs/common';
import { KafkaConsumer } from '@nestjs/microservices';
import { GrpcClient } from './grpc-client';

@Injectable()
export class FraudConsumer {
  constructor(private readonly consumer: KafkaConsumer, private readonly grpc: GrpcClient) {}

  @MessagePattern('orders.created')
  async handle(event: any) {
    const score = await this.grpc.predict({
      text: `${event.customerId} ${event.amount} ${event.ip}`,
    });
    const verdict = score &gt; 0.8 ? 'reject' : 'accept';
    await this.consumer.send({
      topic: 'orders.fraud',
      messages: [{ key: String(event.id), value: JSON.stringify({ ...event, verdict }) }],
    });
  }
}
</code></pre>
<p>The inference container is stateless and can be autoscaled based on a Prometheus metric that tracks request latency. Kubernetes Horizontal Pod Autoscaler (HPA) uses a custom metric <code>inference_latency_ms</code> to add pods when the 95thâ€‘percentile exceeds 120â€¯ms.</p>
<h2 id="npmpackageversioningstrategy">npm Package Versioning Strategy</h2>
<p>All shared librariesâ€”DTO definitions, validation schemas, and Kafka utilitiesâ€”are published as scoped npm packages. Semantic versioning is enforced with a <code>release-it</code> CI job that bumps the patch version on every merge to <code>main</code>. Consumers pin to <code>^1.2.3</code> to receive backwardâ€‘compatible updates automatically, while major releases trigger a migration checklist.</p>
<pre><code>// package.json snippet
{
  "name": "@shop/common",
  "version": "1.4.0",
  "private": false,
  "publishConfig": { "access": "public" },
  "scripts": { "release": "release-it" }
}
</code></pre>
<p>Versioned contracts guarantee that schemaâ€‘aware routing in the readâ€‘side services can safely deserialize events even when fields are added in a later release.</p>
<h2 id="benchmarkingandautoscaling">Benchmarking and Autoscaling</h2>
<p>We measured replication lag, Kafka consumer throughput, and inference latency under three traffic profiles: baseline (100â€¯rps), burst (1â€¯k rps), and spike (5â€¯k rps). Results are summarized below.</p>
<pre><code>Metric                     Baseline   Burst   Spike
---------------------------------------------------
Replication lag (ms)       12         35      78
Kafka consumer lag (msg)   2          15      62
Inference latency p95 (ms) 85         112     210
</code></pre>
<p>During spikes the HPA scaled the inference pods from 2 to 8 within 30â€¯seconds, keeping p95 latency under the 250â€¯ms SLA. Backâ€‘pressure was applied by pausing the Kafka consumer when the internal queue exceeded 10â€¯k messages, preventing OOM crashes.</p>
<h2 id="tradeoffsandcommonpitfalls">Tradeâ€‘offs and Common Pitfalls</h2>
<ul>
<li><strong>Idempotent event handling</strong> â€“ Without deduplication, a replayed <code>orders.created</code> event can create duplicate rows. The consumer stores processed event IDs in a lightweight Redis set with a TTL equal to the retention period.</li>
<li><strong>Schema drift</strong> â€“ Adding a nonâ€‘nullable column to the <code>orders</code> table without a default breaks logical replication. The safe path is to add the column as nullable, backâ€‘fill data, then alter to NOT NULL.</li>
<li><strong>Network partitions</strong> â€“ If the replica loses connectivity, it will fall behind. A watchdog process monitors <code>pg_stat_replication</code> and triggers a resynchronization when lag exceeds 5â€¯seconds.</li>
<li><strong>LLM cold start</strong> â€“ The first inference request after a pod restart incurs a 1â€‘second warmâ€‘up. Preâ€‘warming pods during scaleâ€‘out mitigates this latency spike.</li>
</ul>
<h2 id="keytakeaways">Key Takeaways</h2>
<ul>
<li>A hybrid CQRS model isolates write concerns while leveraging PostgreSQL logical replication for strongly consistent read models.</li>
<li>NestJS microservices combined with Kafka provide ordered, durable event transport without coupling services to the database.</li>
<li>Stateless LLM inference can be autoscaled based on custom latency metrics, delivering realâ€‘time fraud scores within tight SLAs.</li>
<li>Strict versioning of shared npm packages and schemaâ€‘aware routing prevent runtime deserialization errors.</li>
<li>Idempotency, careful schema migrations, and proactive monitoring are essential to avoid data anomalies in an eventâ€‘driven pipeline.</li>
</ul>

  <!-- Footer -->
  <div class="footer">
    <div class="signature">
      <strong>Chinmay Ku Jena</strong><br>
      Distributed Systems & Backend Engineering<br><br>
      ðŸ“± <a href="https://wa.me/918926215167?text=Hi%20Chinmay%2C%20I%20read%20your%20blog%20and%20wanted%20to%20connect." target="_blank">
        Message on WhatsApp
      </a><br>
      ðŸ“§ <a href="mailto:chinmay09jena@gmail.com">
        chinmay09jena@gmail.com
      </a><br>
    </div>
  </div>

</div>

</body>
</html>