<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<style>
body {
  margin: 0;
  padding: 0;
  background-color: #f6f8fa;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
}

.wrapper {
  max-width: 760px;
  margin: 40px auto;
  background: #ffffff;
  padding: 48px;
  border-radius: 12px;
  box-shadow: 0 8px 24px rgba(0,0,0,0.06);
  line-height: 1.7;
  color: #111827;
}

h1 {
  font-size: 34px;
  margin-bottom: 12px;
  line-height: 1.3;
}

.meta {
  font-size: 14px;
  color: #6b7280;
  margin-bottom: 20px;
}

.description {
  font-size: 18px;
  color: #374151;
  margin-bottom: 28px;
}

.tags {
  margin-bottom: 32px;
}

.tag {
  display: inline-block;
  background: #eef2ff;
  color: #3730a3;
  padding: 6px 12px;
  border-radius: 999px;
  font-size: 13px;
  margin-right: 6px;
  margin-bottom: 6px;
}

h2 {
  margin-top: 36px;
  font-size: 24px;
  border-bottom: 1px solid #e5e7eb;
  padding-bottom: 6px;
}

h3 {
  margin-top: 28px;
  font-size: 18px;
}

p {
  margin: 16px 0;
}

code {
  background: #f3f4f6;
  padding: 4px 8px;
  border-radius: 6px;
  font-size: 14px;
  font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
}

pre {
  background: #0f172a;
  color: #f8fafc;
  padding: 18px;
  border-radius: 10px;
  overflow-x: auto;
  font-size: 14px;
  line-height: 1.6;
}

pre code {
  background: none;
  padding: 0;
  color: inherit;
}

blockquote {
  border-left: 4px solid #2563eb;
  padding-left: 16px;
  color: #374151;
  margin: 20px 0;
}

table {
  border-collapse: collapse;
  width: 100%;
  margin: 24px 0;
}

th, td {
  border: 1px solid #e5e7eb;
  padding: 10px;
  text-align: left;
  font-size: 14px;
}

th {
  background: #f9fafb;
}

hr {
  margin: 48px 0;
  border: none;
  border-top: 1px solid #e5e7eb;
}

.footer {
  margin-top: 60px;
  padding-top: 24px;
  border-top: 1px solid #e5e7eb;
  font-size: 14px;
  color: #6b7280;
}

.footer strong {
  color: #111827;
}

.footer a {
  color: #2563eb;
  text-decoration: none;
}

.footer a:hover {
  text-decoration: underline;
}

.signature {
  margin-top: 8px;
  line-height: 1.6;
}
</style>
</head>

<body>

<div class="wrapper">

  <!-- Title -->
  <h1>Designing a Scalable, AI‚ÄëAugmented Event‚ÄëSourced Microservice with NestJS, PostgreSQL Logical Replication, and Vector Search</h1>

  <!-- Meta -->
  <div class="meta">
    üìÖ 2026-02-17
  </div>

  <!-- Description -->
  <div class="description">
    Explores how to build a production‚Äëgrade event‚Äësourced microservice in NestJS that uses PostgreSQL logical replication for durability, integrates LLM‚Äëgenerated vector embeddings for semantic queries, and reconciles legacy npm tooling with modern monorepo workflows.
  </div>

  <!-- Tags -->
  <div class="tags">
    <span class="tag">backend-architecture</span>
    <span class="tag">postgresql</span>
    <span class="tag">nestjs</span>
    <span class="tag">event-sourcing</span>
    <span class="tag">ai-integration</span>
    <span class="tag">vector-search</span>
    <span class="tag">npm</span>
  </div>

  <!-- Blog Content (HTML) -->
  <p>In large‚Äëscale systems the write path must remain cheap while the read side can afford richer processing. Event sourcing satisfies the first requirement by persisting immutable facts, and it enables replayable projections that can be enriched with AI‚Äëgenerated semantics. Combining this pattern with PostgreSQL logical replication gives a proven durability layer, while a vector‚Äësearch extension such as pgvector supplies fast semantic lookup without abandoning the relational ecosystem.</p>
<p>The article walks through a production‚Äëgrade NestJS service that implements an event‚Äësourced domain, replicates events to a read‚Äëmodel database, and augments that model with LLM‚Äëderived embeddings. It also shows how to reconcile legacy npm scripts with a monorepo managed by Nx, keeping the developer experience smooth while scaling to many microservices.</p>
<p>Key challenges include handling write‚Äëheavy streams without saturating the replication slot, mitigating lag between the write and read databases, and preventing AI inference from becoming a synchronous bottleneck. The solution presented isolates inference behind an async queue, applies back‚Äëpressure, and scales the vector index independently.</p>
<h2 id="eventsourceddomainmodel">Event‚ÄëSourced Domain Model</h2>
<p>The domain revolves around a <code>User</code> aggregate. Every state change is captured as an event class. NestJS modules expose command handlers that validate intent and emit events to an <code>EventStore</code> backed by PostgreSQL's <code>wal2json</code> logical decoding.</p>
<pre><code>// src/events/user-created.event.ts
export class UserCreatedEvent {
    constructor(public readonly userId: string, public readonly payload: any) {}
}

// src/commands/create-user.command.ts
export class CreateUserCommand {
    constructor(public readonly userId: string, public readonly data: any) {}
}
</code></pre>
<p>Command handlers persist the event inside a transaction and publish it to a Kafka topic. The same topic feeds the projection service and the AI enrichment worker.</p>
<h2 id="hybridstoragelayer">Hybrid Storage Layer</h2>
<p>Two databases cooperate:</p>
<ol>
<li><strong>Primary PostgreSQL</strong> ‚Äì stores the raw event stream in an <code>events</code> table and runs logical replication slots.</li>
<li><strong>Read‚Äëmodel PostgreSQL</strong> ‚Äì contains denormalized tables (<code>users</code>, <code>user_vectors</code>) and a <code>pgvector</code> column for embeddings.</li>
</ol>
<p>Logical replication streams <code>INSERT</code> statements from the primary to the read model. A small trigger on the <code>users</code> table extracts the latest state and writes it to a materialized view that the vector index consumes.</p>
<pre><code>-- Trigger on primary to forward new events
CREATE FUNCTION forward_event() RETURNS trigger AS $
BEGIN
    PERFORM pg_notify('event_channel', row_to_json(NEW)::text);
    RETURN NEW;
END;
$ LANGUAGE plpgsql;

CREATE TRIGGER event_forward AFTER INSERT ON events
FOR EACH ROW EXECUTE FUNCTION forward_event();
</code></pre>
<p>The read model adds a <code>vector</code> column of type <code>vector(1536)</code> (compatible with OpenAI embeddings). Indexing uses the <code>ivfflat</code> method for efficient ANN queries.</p>
<pre><code>CREATE INDEX user_vector_idx ON user_vectors USING ivfflat (embedding vector) WITH (lists = 100);
</code></pre>
<h2 id="nestjsmodulesforcommandprojectionandaienrichment">NestJS Modules for Command, Projection, and AI Enrichment</h2>
<p>The service is split into three NestJS modules:</p>
<ul>
<li><p><strong>CommandModule</strong> ‚Äì validates input, creates events, and writes to the primary DB.</p></li>
<li><p><strong>ProjectionModule</strong> ‚Äì consumes the replication stream, updates denormalized tables, and triggers vector recomputation.</p></li>
<li><p><strong>AiEnrichmentModule</strong> ‚Äì runs in a separate worker process, pulls pending user IDs from a Redis queue, calls the LLM embedding API, and writes the result back to <code>user_vectors</code>.</p>
<p>// src/ai-enrichment/ai-worker.service.ts<br />
@Injectable()<br />
export class AiWorkerService {<br />
    constructor(@InjectQueue('ai') private readonly queue: Queue,<br />
                private readonly http: HttpService,<br />
                private readonly repo: UserVectorRepository) {}</p>
<pre><code>async process(job: Job&lt;string&gt;) {
    const userId = job.data;
    const profile = await this.repo.findProfile(userId);
    const response = await this.http.post('https://api.openai.com/v1/embeddings', {
        input: profile.bio,
        model: 'text-embedding-3-large'
    }).toPromise();
    const embedding = response.data.data[0].embedding;
    await this.repo.saveEmbedding(userId, embedding);
}
</code></pre>
<p>}</p></li>
</ul>
<p>The queue is configured with a concurrency limit and a rate‚Äëlimiter that respects the LLM provider‚Äôs quota. If the queue backs up, the projection service pauses consumption, preventing unbounded memory growth.</p>
<h2 id="performancetuning">Performance Tuning</h2>
<h3 id="writeheavyeventstreams">Write‚ÄëHeavy Event Streams</h3>
<ul>
<li>Batch inserts into the <code>events</code> table (default 100 events per transaction).</li>
<li>Use <code>UNLOGGED</code> tables for temporary staging when durability can be deferred.</li>
<li>Keep the logical replication slot‚Äôs <code>max_replication_slots</code> high enough to avoid slot eviction.</li>
</ul>
<h3 id="replicationlagmitigation">Replication Lag Mitigation</h3>
<ul>
<li>Monitor <code>pg_stat_replication</code> and set <code>wal_sender_timeout</code> to a low value.</li>
<li>Deploy a small ‚Äúlag‚Äëwatcher‚Äù service that throttles incoming commands when <code>pg_last_wal_replay_lsn</code> falls behind.</li>
</ul>
<h3 id="vectorindexscaling">Vector Index Scaling</h3>
<ul>
<li>Periodically <code>REFRESH MATERIALIZED VIEW user_vectors_mv</code> to rebuild the IVF index after a bulk of embeddings arrive.</li>
<li>Partition <code>user_vectors</code> by creation month to keep each segment size manageable.</li>
</ul>
<h2 id="commonpitfalls">Common Pitfalls</h2>
<p>Treating AI inference as a synchronous request inside the command path is the most frequent mistake. The latency of an LLM call can vary from hundreds of milliseconds to several seconds, and a burst of commands will saturate the HTTP client pool, causing timeouts and cascading back‚Äëpressure into the write side. The pattern shown isolates inference behind a queue, applies exponential back‚Äëoff, and makes the command path return immediately after persisting the event.</p>
<p>Another subtle issue is assuming replication lag is negligible. In a multi‚Äëregion deployment, network jitter can cause the read model to be seconds behind, leading to stale search results. The solution is to expose the lag metric in health checks and, if necessary, fallback to a cached snapshot for read‚Äëonly queries.</p>
<h2 id="keytakeaways">Key Takeaways</h2>
<ul>
<li>Event sourcing decouples write durability from read‚Äëside enrichment, allowing AI processing to run asynchronously.</li>
<li>PostgreSQL logical replication provides a low‚Äëlatency, strongly consistent pipe between the event store and the vector‚Äëenabled read model.</li>
<li>Queue‚Äëbased AI inference prevents back‚Äëpressure from spilling into the command path and keeps resource usage predictable.</li>
<li>Careful monitoring of replication lag and vector index health is essential for a smooth production experience.</li>
</ul>

  <!-- Footer -->
  <div class="footer">
    <div class="signature">
      <strong>Chinmay Ku Jena</strong><br>
      Distributed Systems & Backend Engineering<br><br>
      üì± <a href="https://wa.me/918926215167?text=Hi%20Chinmay%2C%20I%20read%20your%20blog%20and%20wanted%20to%20connect." target="_blank">
        Message on WhatsApp
      </a><br>
      üìß <a href="mailto:chinmay09jena@gmail.com">
        chinmay09jena@gmail.com
      </a><br>
    </div>
  </div>

</div>

</body>
</html>