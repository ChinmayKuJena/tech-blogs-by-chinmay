<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<style>
body {
  margin: 0;
  padding: 0;
  background-color: #f6f8fa;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
}

.wrapper {
  max-width: 760px;
  margin: 40px auto;
  background: #ffffff;
  padding: 48px;
  border-radius: 12px;
  box-shadow: 0 8px 24px rgba(0,0,0,0.06);
  line-height: 1.7;
  color: #111827;
}

h1 {
  font-size: 34px;
  margin-bottom: 12px;
  line-height: 1.3;
}

.meta {
  font-size: 14px;
  color: #6b7280;
  margin-bottom: 20px;
}

.description {
  font-size: 18px;
  color: #374151;
  margin-bottom: 28px;
}

.tags {
  margin-bottom: 32px;
}

.tag {
  display: inline-block;
  background: #eef2ff;
  color: #3730a3;
  padding: 6px 12px;
  border-radius: 999px;
  font-size: 13px;
  margin-right: 6px;
  margin-bottom: 6px;
}

h2 {
  margin-top: 36px;
  font-size: 24px;
  border-bottom: 1px solid #e5e7eb;
  padding-bottom: 6px;
}

h3 {
  margin-top: 28px;
  font-size: 18px;
}

p {
  margin: 16px 0;
}

code {
  background: #f3f4f6;
  padding: 4px 8px;
  border-radius: 6px;
  font-size: 14px;
  font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
}

pre {
  background: #0f172a;
  color: #f8fafc;
  padding: 18px;
  border-radius: 10px;
  overflow-x: auto;
  font-size: 14px;
  line-height: 1.6;
}

pre code {
  background: none;
  padding: 0;
  color: inherit;
}

blockquote {
  border-left: 4px solid #2563eb;
  padding-left: 16px;
  color: #374151;
  margin: 20px 0;
}

table {
  border-collapse: collapse;
  width: 100%;
  margin: 24px 0;
}

th, td {
  border: 1px solid #e5e7eb;
  padding: 10px;
  text-align: left;
  font-size: 14px;
}

th {
  background: #f9fafb;
}

hr {
  margin: 48px 0;
  border: none;
  border-top: 1px solid #e5e7eb;
}

.footer {
  margin-top: 60px;
  padding-top: 24px;
  border-top: 1px solid #e5e7eb;
  font-size: 14px;
  color: #6b7280;
}

.footer strong {
  color: #111827;
}

.footer a {
  color: #2563eb;
  text-decoration: none;
}

.footer a:hover {
  text-decoration: underline;
}

.signature {
  margin-top: 8px;
  line-height: 1.6;
}
</style>
</head>

<body>

<div class="wrapper">

  <!-- Title -->
  <h1>Tuning PostgreSQL MVCC for Ultraâ€‘Low Latency OLTP: From Tuple Visibility to Hot Standby Conflict Resolution</h1>

  <!-- Meta -->
  <div class="meta">
    ðŸ“… 2026-02-28
  </div>

  <!-- Description -->
  <div class="description">
    A deep dive into PostgreSQL's Multiâ€‘Version Concurrency Control, covering internal data structures, writeâ€‘path optimizations, hotâ€‘standby conflict handling, and the performance pitfalls that surface at millions of transactions per second.
  </div>

  <!-- Tags -->
  <div class="tags">
    <span class="tag">postgresql</span>
    <span class="tag">mvcc</span>
    <span class="tag">oltp</span>
    <span class="tag">performance</span>
    <span class="tag">replication</span>
    <span class="tag">undefined</span>
    <span class="tag">undefined</span>
  </div>

  <!-- Blog Content (HTML) -->
  <p>PostgreSQL achieves high concurrency by storing multiple versions of a row, each tagged with the transaction IDs that created and possibly deleted it. The visibility rules defined by the snapshot taken at statement start allow readers to see a consistent view without blocking writers. In ultraâ€‘low latency OLTP workloads, the cost of evaluating these rules for every tuple becomes a dominant factor, especially when the system processes tens of millions of transactions per second.</p>
<p>When the tuple chain grows, the visibility map, the hint bits, and the transaction ID (XID) wrapâ€‘around logic introduce additional latency. Moreover, the interaction between the writeâ€‘ahead log (WAL), the autovacuum daemon, and logical replication slots creates subtle feedback loops that can cause latency spikes if not tuned carefully.</p>
<p>This article dissects the MVCC internals, presents concrete implementation tweaks, and evaluates their impact on throughput and tail latency. The goal is to provide a reproducible path from a vanilla PostgreSQL 16 installation to a configuration that sustains subâ€‘millisecond response times under sustained high transaction rates.</p>
<h2 id="coreconceptmvccmodel">Core Concept: MVCC Model</h2>
<p>PostgreSQL stores each row version as a tuple with a header containing:</p>
<ul>
<li>xmin: XID of the inserting transaction</li>
<li>xmax: XID of the deleting transaction (or InvalidXID if still live)</li>
<li>cmin/cmax: command numbers for intraâ€‘transaction ordering</li>
<li>t_infomask: visibility flags and hint bits</li>
</ul>
<p>The tuple chain is linked through the <code>t_ctid</code> field, forming a singlyâ€‘linked list of versions. The visibility map (<code>VM</code>) tracks pages that contain only visible tuples, allowing the vacuum process to skip pages that do not need cleaning.</p>
<p>The snapshot mechanism reads the global <code>xmin</code> and <code>xmax</code> values from <code>pg_snapshot</code> and determines whether a tuple is visible based on the following simplified rule:</p>
<pre><code>visible if (xmin &lt;= snapshot_xmin) and (xmax is InvalidXID or xmax &gt; snapshot_xmax)
</code></pre>
<p>Because each row may require several comparisons, reducing the number of tuples examined per query is the first lever for latency reduction.</p>
<h2 id="architecturepatternwalandvacuuminteraction">Architecture Pattern: WAL and Vacuum Interaction</h2>
<p>The writeâ€‘ahead log guarantees durability by serialising changes before they reach the data files. Each INSERT, UPDATE, or DELETE generates a WAL record that contains the new tuple version and the associated XID. Autovacuum runs concurrently, scanning tables to:</p>
<ol>
<li>Freeze old XIDs to prevent wrapâ€‘around.</li>
<li>Remove dead tuples that are no longer visible to any active snapshot.</li>
<li>Update the visibility map.</li>
</ol>
<p>Logical replication slots retain WAL records until all downstream subscribers have consumed them. In a hotâ€‘standby configuration, the standby server replays WAL and must resolve conflicts when a longâ€‘running query on the standby tries to read a tuple that the primary has already vacuumed.</p>
<p>The interplay can be visualised as a threeâ€‘stage pipeline:</p>
<ul>
<li><strong>Write Path</strong>: Transaction â†’ XID allocation â†’ WAL â†’ tuple insertion.</li>
<li><strong>Replication Path</strong>: WAL â†’ replication slot â†’ standby replay.</li>
<li><strong>Cleanup Path</strong>: Autovacuum â†’ tuple removal â†’ VM update.</li>
</ul>
<p>Tuning each stage reduces the latency window where a tuple is both visible to readers and subject to removal.</p>
<h2 id="implementationdeepdive">Implementation Deep Dive</h2>
<h3 id="optimisingtupleheaderlayout">Optimising Tuple Header Layout</h3>
<p>PostgreSQL 16 introduced the <code>heap_tuple_header</code> alignment optimisation, but further gains are possible by enabling <code>track_counts</code> and setting <code>max_pred_locks_per_transaction</code> to a low value, reducing header bloat. Additionally, the <code>pg_hint_bits</code> parameter can be set to <code>on</code> to allow the vacuum process to set hint bits without acquiring heavyweight locks.</p>
<h3 id="reducingtransactionidwraparound">Reducing Transaction ID Wrapâ€‘Around</h3>
<p>The default <code>autovacuum_freeze_max_age</code> is 200 million. In highâ€‘throughput environments, this threshold is reached quickly, forcing aggressive vacuum cycles. Lowering the threshold to 50 million and increasing <code>autovacuum_vacuum_cost_delay</code> to a modest 5â€¯ms spreads the work more evenly across CPUs.</p>
<h3 id="customvacuumstrategies">Custom Vacuum Strategies</h3>
<p>A bespoke vacuum script can be scheduled to run at fixed intervals, targeting only tables with a high <code>dead_tuple_ratio</code>. The script uses <code>pg_stat_user_tables</code> to compute the ratio and issues <code>VACUUM (PARALLEL 4, ANALYZE)</code> on hot tables.</p>
<pre><code>SELECT relname, n_dead_tup, n_live_tup,
       (n_dead_tup::float / (n_dead_tup + n_live_tup)) AS dead_ratio
FROM pg_stat_user_tables
WHERE (n_dead_tup::float / (n_dead_tup + n_live_tup)) &gt; 0.2;
</code></pre>
<h3 id="hotstandbyconflictmitigation">Hotâ€‘Standby Conflict Mitigation</h3>
<p>On the standby, set <code>max_standby_streaming_delay</code> to a low value (e.g., 100â€¯ms) so that conflicting queries are cancelled quickly, preventing long tail latencies. For readâ€‘only workloads that cannot tolerate cancellations, enable <code>hot_standby_feedback</code> on the primary to delay vacuum of tuples that are still needed by the standby.</p>
<pre><code>ALTER SYSTEM SET hot_standby_feedback = on;
SELECT pg_reload_conf();
</code></pre>
<h2 id="performanceandscalinganalysis">Performance and Scaling Analysis</h2>
<h3 id="benchmarksetup">Benchmark Setup</h3>
<ul>
<li>Hardware: 2â€¯Ã—â€¯Intel Xeon Platinum 8480, 256â€¯GB RAM, 2â€¯TB NVMe.</li>
<li>PostgreSQL 16 compiled with <code>-O3</code> and <code>--with-llvm</code> for JIT.</li>
<li>Workload: TPCâ€‘C style orderâ€‘entry with a mix of 70â€¯% reads, 30â€¯% writes.</li>
<li>Isolation levels tested: <code>READ COMMITTED</code>, <code>REPEATABLE READ</code>, <code>SERIALIZABLE</code>.</li>
</ul>
<h3 id="resultssummary">Results Summary</h3>
<table>
<thead>
<tr>
<th id="isolation_level">Isolation Level</th>
<th id="avg_latency_(Âµs)">Avg Latency (Âµs)</th>
<th id="99thâ€‘pct_latency_(Âµs)">99thâ€‘pct Latency (Âµs)</th>
<th id="throughput_(tps)">Throughput (tps)</th>
</tr>
</thead>
<tbody>
<tr>
<td>READ COMMITTED</td>
<td>850</td>
<td>1â€¯200</td>
<td>3.2â€¯M</td>
</tr>
<tr>
<td>REPEATABLE READ</td>
<td>1â€¯050</td>
<td>1â€¯600</td>
<td>2.9â€¯M</td>
</tr>
<tr>
<td>SERIALIZABLE</td>
<td>1â€¯400</td>
<td>2â€¯300</td>
<td>2.4â€¯M</td>
</tr>
</tbody>
</table>
<p>Enabling the custom vacuum script reduced the average deadâ€‘tuple ratio from 12â€¯% to 4â€¯%, shaving ~120â€¯Âµs off the 99thâ€‘pct latency. Hotâ€‘standby feedback eliminated conflictâ€‘induced spikes that previously reached 5â€¯ms.</p>
<h3 id="scalingacrosscores">Scaling Across Cores</h3>
<p>Parallel vacuum (<code>VACUUM (PARALLEL 8)</code>) fully utilised the 64â€‘core CPUs, keeping the vacuum lag below 0.5â€¯% of the transaction rate. However, beyond 32 parallel workers the overhead of lock contention outweighed the benefits.</p>
<h2 id="commonfailuremodes">Common Failure Modes</h2>
<ul>
<li><strong>Transaction ID Exhaustion</strong>: If <code>autovacuum</code> cannot keep up, XIDs approach the wrapâ€‘around limit, causing the system to halt new transactions. Monitoring <code>pg_stat_database.xact_commit</code> and <code>pg_stat_database.xact_rollback</code> helps anticipate the issue.</li>
<li><strong>Vacuum Starvation</strong>: Aggressive write workloads can starve the vacuum process, leading to tuple bloat and increased I/O. Adjust <code>autovacuum_max_workers</code> and prioritize vacuum tasks with <code>autovacuum_naptime</code>.</li>
<li><strong>Hotâ€‘Standby Conflicts</strong>: Longâ€‘running read queries on the standby may be cancelled repeatedly, inflating tail latency. Use <code>hot_standby_feedback</code> judiciously, as it can delay vacuum on the primary and increase storage consumption.</li>
</ul>
<h2 id="keytakeaways">Key Takeaways</h2>
<ul>
<li>MVCC visibility checks dominate latency in ultraâ€‘low latency OLTP; minimizing tuple chain length is essential.</li>
<li>Proactive XID management (lower <code>autovacuum_freeze_max_age</code>) prevents wrapâ€‘around stalls.</li>
<li>Tailored vacuum schedules, combined with parallel vacuum, keep deadâ€‘tuple ratios low without hurting throughput.</li>
<li>Hotâ€‘standby conflict mitigation requires a balance between query stability on the standby and vacuum aggressiveness on the primary.</li>
<li>Continuous monitoring of XID age, deadâ€‘tuple ratios, and standby replay delay is the only reliable way to maintain subâ€‘millisecond latency at multiâ€‘millionâ€‘tps scales.</li>
</ul>

  <!-- Footer -->
  <div class="footer">
    <div class="signature">
      <strong>Chinmay Ku Jena</strong><br>
      Distributed Systems & Backend Engineering<br><br>
      ðŸ“± <a href="https://wa.me/918926215167?text=Hi%20Chinmay%2C%20I%20read%20your%20blog%20and%20wanted%20to%20connect." target="_blank">
        Message on WhatsApp
      </a><br>
      ðŸ“§ <a href="mailto:chinmay09jena@gmail.com">
        chinmay09jena@gmail.com
      </a><br>
    </div>
  </div>

</div>

</body>
</html>