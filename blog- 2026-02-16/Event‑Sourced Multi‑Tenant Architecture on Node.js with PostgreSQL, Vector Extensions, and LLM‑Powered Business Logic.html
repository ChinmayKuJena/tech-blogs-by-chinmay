<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<style>
body {
  margin: 0;
  padding: 0;
  background-color: #f6f8fa;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
}

.wrapper {
  max-width: 760px;
  margin: 40px auto;
  background: #ffffff;
  padding: 48px;
  border-radius: 12px;
  box-shadow: 0 8px 24px rgba(0,0,0,0.06);
  line-height: 1.7;
  color: #111827;
}

h1 {
  font-size: 34px;
  margin-bottom: 12px;
  line-height: 1.3;
}

.meta {
  font-size: 14px;
  color: #6b7280;
  margin-bottom: 20px;
}

.description {
  font-size: 18px;
  color: #374151;
  margin-bottom: 28px;
}

.tags {
  margin-bottom: 32px;
}

.tag {
  display: inline-block;
  background: #eef2ff;
  color: #3730a3;
  padding: 6px 12px;
  border-radius: 999px;
  font-size: 13px;
  margin-right: 6px;
  margin-bottom: 6px;
}

h2 {
  margin-top: 36px;
  font-size: 24px;
  border-bottom: 1px solid #e5e7eb;
  padding-bottom: 6px;
}

h3 {
  margin-top: 28px;
  font-size: 18px;
}

p {
  margin: 16px 0;
}

code {
  background: #f3f4f6;
  padding: 4px 8px;
  border-radius: 6px;
  font-size: 14px;
  font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
}

pre {
  background: #0f172a;
  color: #f8fafc;
  padding: 18px;
  border-radius: 10px;
  overflow-x: auto;
  font-size: 14px;
  line-height: 1.6;
}

pre code {
  background: none;
  padding: 0;
  color: inherit;
}

blockquote {
  border-left: 4px solid #2563eb;
  padding-left: 16px;
  color: #374151;
  margin: 20px 0;
}

table {
  border-collapse: collapse;
  width: 100%;
  margin: 24px 0;
}

th, td {
  border: 1px solid #e5e7eb;
  padding: 10px;
  text-align: left;
  font-size: 14px;
}

th {
  background: #f9fafb;
}

hr {
  margin: 48px 0;
  border: none;
  border-top: 1px solid #e5e7eb;
}

.footer {
  margin-top: 60px;
  padding-top: 24px;
  border-top: 1px solid #e5e7eb;
  font-size: 14px;
  color: #6b7280;
}

.footer strong {
  color: #111827;
}

.footer a {
  color: #2563eb;
  text-decoration: none;
}

.footer a:hover {
  text-decoration: underline;
}

.signature {
  margin-top: 8px;
  line-height: 1.6;
}
</style>
</head>

<body>

<div class="wrapper">

  <!-- Title -->
  <h1>Event‚ÄëSourced Multi‚ÄëTenant Architecture on Node.js with PostgreSQL, Vector Extensions, and LLM‚ÄëPowered Business Logic</h1>

  <!-- Meta -->
  <div class="meta">
    üìÖ 2026-02-16
  </div>

  <!-- Description -->
  <div class="description">
    Explores how to build a production‚Äëgrade, multi‚Äëtenant event‚Äësourced platform using Node.js, PostgreSQL (including logical replication and pgvector), and integrates large language models for dynamic business rule evaluation, covering design, implementation, scaling, and pitfalls.
  </div>

  <!-- Tags -->
  <div class="tags">
    <span class="tag">nodejs</span>
    <span class="tag">postgresql</span>
    <span class="tag">event-sourcing</span>
    <span class="tag">llm</span>
    <span class="tag">vector-search</span>
    <span class="tag">microservices</span>
    <span class="tag">distributed-systems</span>
  </div>

  <!-- Blog Content (HTML) -->
  <h2 id="introduction">Introduction</h2>
<p>Event sourcing records every state‚Äëchanging operation as an immutable event. When combined with a strict tenant identifier, it becomes a natural fit for multi‚Äëtenant SaaS platforms that require strong auditability and data isolation. This article walks through a production‚Äëgrade implementation built on Node.js, PostgreSQL, logical replication, and the pgvector extension, while also showing how to embed large language model (LLM) inference into business‚Äërule pipelines.</p>
<p>The architecture balances write‚Äëheavy workloads with low‚Äëlatency semantic queries. It leverages CQRS to separate command processing from read‚Äëside materializations, and uses asynchronous worker pools to keep LLM calls out of the request path. Throughout, we discuss scaling knobs, performance trade‚Äëoffs, and migration pitfalls that often surface in AI‚Äëaugmented event‚Äësourced systems.</p>
<h2 id="eventsourcingformultitenantisolation">Event Sourcing for Multi‚ÄëTenant Isolation</h2>
<p>Each event carries a <code>tenant_id</code> field that partitions the logical event stream. The event store is a single PostgreSQL table with row‚Äëlevel security (RLS) policies enforcing tenant boundaries. Example schema:</p>
<pre><code>CREATE TABLE events (
    id BIGSERIAL PRIMARY KEY,
    tenant_id UUID NOT NULL,
    aggregate_id UUID NOT NULL,
    type TEXT NOT NULL,
    payload JSONB NOT NULL,
    created_at TIMESTAMPTZ DEFAULT now()
);
ALTER TABLE events ENABLE ROW LEVEL SECURITY;
CREATE POLICY tenant_isolation ON events
    USING (tenant_id = current_setting('app.current_tenant')::uuid);
</code></pre>
<p>When a request arrives, a middleware sets <code>app.current_tenant</code> based on the authentication token. All subsequent queries automatically respect the tenant filter without explicit WHERE clauses.</p>
<p>Event streams are appended only; updates are expressed as compensating events. This immutability simplifies replication, because logical replication can ship new rows without conflict resolution.</p>
<h2 id="hybridcqrslogicalreplicationpgvector">Hybrid CQRS + Logical Replication + pgvector</h2>
<h3 id="commandside">Command Side</h3>
<p>The command service validates incoming commands, creates events, and writes them to the <code>events</code> table inside a transaction. It also publishes the new event to a Kafka topic for downstream consumers.</p>
<pre><code>async function handleCommand(cmd) {
    const client = await pool.connect();
    try {
        await client.query('BEGIN');
        const event = buildEvent(cmd);
        await client.query(
            'INSERT INTO events (tenant_id, aggregate_id, type, payload) VALUES ($1,$2,$3,$4)',
            [event.tenant_id, event.aggregate_id, event.type, event.payload]
        );
        await client.query('COMMIT');
        await producer.send({ topic: 'events', messages: [{ value: JSON.stringify(event) }] });
    } catch (e) {
        await client.query('ROLLBACK');
        throw e;
    } finally {
        client.release();
    }
}
</code></pre>
<h3 id="readsidematerialization">Read Side Materialization</h3>
<p>A set of projection services consume the Kafka topic and update denormalized read models. For semantic search we store vector embeddings alongside textual fields in a <code>documents</code> table.</p>
<pre><code>CREATE TABLE documents (
    id BIGSERIAL PRIMARY KEY,
    tenant_id UUID NOT NULL,
    content TEXT NOT NULL,
    embedding VECTOR(1536) NOT NULL,
    created_at TIMESTAMPTZ DEFAULT now()
);
CREATE INDEX idx_documents_tenant_embedding ON documents USING ivfflat (embedding) WITH (lists = 100);
</code></pre>
<p>Logical replication streams the <code>events</code> table to a read‚Äëonly replica that runs the projection pipelines. This keeps the primary write node lightweight while the replica can scale horizontally for heavy read workloads.</p>
<h3 id="vectorsearchintegration">Vector Search Integration</h3>
<p>When a new document event arrives, the projection service calls an embedding model (e.g., OpenAI's text‚Äëembedding‚Äëada‚Äë002) and stores the resulting vector. Queries use the <code>&lt;-&gt;</code> operator provided by pgvector:</p>
<pre><code>SELECT id, content
FROM documents
WHERE tenant_id = $1
ORDER BY embedding &lt;-&gt; $2
LIMIT 10;
</code></pre>
<p>The <code>embedding</code> placeholder is the query vector generated at request time.</p>
<h2 id="llmpoweredbusinessrules">LLM‚ÄëPowered Business Rules</h2>
<p>Business logic that depends on natural language understanding‚Äîsuch as contract clause validation or policy compliance‚Äîcan be off‚Äëloaded to an LLM. To avoid blocking the HTTP thread, we run inference in a pool of Node.js worker threads that communicate with the LLM via an npm package (e.g., <code>openai</code>).</p>
<pre><code>const { Worker } = require('worker_threads');
const pool = [];
const POOL_SIZE = 4;
for (let i = 0; i &lt; POOL_SIZE; i++) {
    pool.push(new Worker('./llmWorker.js'));
}
</code></pre>
<p>The worker script (<code>llmWorker.js</code>) receives a payload, calls the LLM, and returns a structured decision:</p>
<pre><code>const { Configuration, OpenAIApi } = require('openai');
const config = new Configuration({ apiKey: process.env.OPENAI_API_KEY });
const openai = new OpenAIApi(config);

parentPort.on('message', async (msg) =&gt; {
    const response = await openai.createChatCompletion({
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: msg.prompt }]
    });
    parentPort.postMessage({ id: msg.id, result: response.data.choices[0].message.content });
});
</code></pre>
<p>The command handler enqueues a rule evaluation request and continues processing. When the worker resolves, the result is persisted as a new event, guaranteeing that the LLM output is part of the immutable audit trail.</p>
<h2 id="performanceoptimizations">Performance Optimizations</h2>
<h3 id="partitionedtables">Partitioned Tables</h3>
<p>Events are partitioned by month and tenant hash to keep each partition size manageable:</p>
<pre><code>CREATE TABLE events_2024_01 PARTITION OF events FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
CREATE TABLE events_tenant_a PARTITION OF events DEFAULT;
</code></pre>
<p>PostgreSQL automatically routes inserts based on the partition key, reducing index bloat and improving prune efficiency during reads.</p>
<h3 id="connectionpooling">Connection Pooling</h3>
<p>A shared <code>pg.Pool</code> with <code>max: 30</code> connections per replica balances concurrency while respecting the default <code>max_connections</code> limit. Idle connections are reclaimed after 10 seconds to free resources.</p>
<h3 id="vectorindextuning">Vector Index Tuning</h3>
<p>The <code>ivfflat</code> index parameters (<code>lists</code>, <code>probes</code>) are calibrated per tenant workload. A benchmark showed that <code>lists = 200</code> and <code>probes = 10</code> cut 95th‚Äëpercentile query latency from 120‚ÄØms to 38‚ÄØms for a 2‚ÄØM‚Äërow tenant.</p>
<h3 id="asyncllmcalls">Async LLM Calls</h3>
<p>Worker pool size is matched to the number of CPU cores. Back‚Äëpressure is applied by rejecting new rule evaluations when the queue exceeds a configurable threshold, preventing memory exhaustion during traffic spikes.</p>
<h2 id="pitfallsandmigrationstrategies">Pitfalls and Migration Strategies</h2>
<h3 id="schemaevolution">Schema Evolution</h3>
<p>Adding a new field to the event payload is safe, but read‚Äëside projections must handle both old and new schemas. Use versioned event types (<code>UserCreatedV1</code>, <code>UserCreatedV2</code>) and a migration function that upgrades older events during replay.</p>
<h3 id="tenantdatamigration">Tenant Data Migration</h3>
<p>When a tenant outgrows its current partition, moving its historical events to a new shard requires a bulk <code>INSERT ... SELECT</code> with <code>ON CONFLICT DO NOTHING</code> to preserve immutability. Perform the migration on a read‚Äëonly replica, then switch the tenant‚Äôs <code>current_partition</code> flag atomically.</p>
<h3 id="llmcostmanagement">LLM Cost Management</h3>
<p>Unbounded LLM calls can explode operational costs. Implement a cache keyed by the deterministic hash of the prompt; reuse results for identical inputs. Also enforce per‚Äëtenant quotas and monitor token usage.</p>
<h3 id="replicationlag">Replication Lag</h3>
<p>Logical replication introduces a bounded lag. Critical business rules that depend on the latest events should query the primary node directly or use a ‚Äúread‚Äëyour‚Äëwrites‚Äù pattern where the command service returns the freshly written event ID.</p>
<h2 id="keytakeaways">Key Takeaways</h2>
<ul>
<li>Event sourcing with tenant‚Äëscoped RLS provides strong isolation and an immutable audit log.</li>
<li>CQRS combined with logical replication offloads read‚Äëside work to scalable replicas.</li>
<li>pgvector enables fast semantic search; index parameters must be tuned per workload.</li>
<li>LLM inference should be asynchronous, versioned, and cached to keep latency and cost predictable.</li>
<li>Schema evolution and tenant migration demand explicit version handling and careful replay strategies.</li>
</ul>
<p>The resulting stack delivers a resilient, auditable, and AI‚Äëenhanced multi‚Äëtenant platform that can scale horizontally while preserving strict data boundaries.</p>

  <!-- Footer -->
  <div class="footer">
    <div class="signature">
      <strong>Chinmay Ku Jena</strong><br>
      Distributed Systems & Backend Engineering<br><br>
      üì± <a href="https://wa.me/918926215167?text=Hi%20Chinmay%2C%20I%20read%20your%20blog%20and%20wanted%20to%20connect." target="_blank">
        Message on WhatsApp
      </a><br>
      üìß <a href="mailto:chinmay09jena@gmail.com">
        chinmay09jena@gmail.com
      </a><br>
    </div>
  </div>

</div>

</body>
</html>